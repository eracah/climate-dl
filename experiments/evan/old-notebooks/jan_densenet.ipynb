{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Creates a DenseNet model in Lasagne, following the paper\n",
    "\"Densely Connected Convolutional Networks\"\n",
    "by Gao Huang, Zhuang Liu, Kilian Q. Weinberger, 2016.\n",
    "https://arxiv.org/abs/1608.06993\n",
    "\n",
    "Author: Jan Schl√ºter\n",
    "\"\"\"\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import (InputLayer, Conv2DLayer, ConcatLayer, DenseLayer,\n",
    "                            DropoutLayer, BatchNormLayer, Pool2DLayer,\n",
    "                            GlobalPoolLayer, NonlinearityLayer)\n",
    "from lasagne.nonlinearities import rectify, softmax\n",
    "\n",
    "def build_densenet(input_shape=(None, 3, 32, 32), input_var=None, classes=10,\n",
    "                   depth=40, first_output=16, growth_rate=12, num_blocks=3,\n",
    "                   dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a DenseNet model in Lasagne.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape : tuple\n",
    "        The shape of the input layer, as ``(batchsize, channels, rows, cols)``.\n",
    "        Any entry except ``channels`` can be ``None`` to indicate free size.\n",
    "    input_var : Theano expression or None\n",
    "        Symbolic input variable. Will be created automatically if not given.\n",
    "    classes : int\n",
    "        The number of classes of the softmax output.\n",
    "    depth : int\n",
    "        Depth of the network. Must be ``num_blocks * n + 1`` for some ``n``.\n",
    "        (Parameterizing by depth rather than n makes it easier to follow the\n",
    "        paper.)\n",
    "    first_output : int\n",
    "        Number of channels of initial convolution before entering the first\n",
    "        dense block, should be of comparable size to `growth_rate`.\n",
    "    growth_rate : int\n",
    "        Number of feature maps added per layer.\n",
    "    num_blocks : int\n",
    "        Number of dense blocks (defaults to 3, as in the original paper).\n",
    "    dropout : float\n",
    "        The dropout rate. Set to zero (the default) to disable dropout.\n",
    "    batchsize : int or None\n",
    "        The batch size to build the model for, or ``None`` (the default) to\n",
    "        allow any batch size.\n",
    "    inputsize : int, tuple of int or None        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    network : Layer instance\n",
    "        Lasagne Layer instance for the output layer.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Gao Huang et al. (2016):\n",
    "           Densely Connected Convolutional Networks.\n",
    "           https://arxiv.org/abs/1608.06993\n",
    "    \"\"\"\n",
    "    if (depth - 1) % num_blocks != 0:\n",
    "        raise ValueError(\"depth must be num_blocks * n + 1 for some n\")\n",
    "    \n",
    "    # input and initial convolution\n",
    "    network = InputLayer(input_shape, input_var, name='input')\n",
    "    network = Conv2DLayer(network, first_output, 3, pad='same',\n",
    "                          W=lasagne.init.HeNormal(gain='relu'),\n",
    "                          b=None, nonlinearity=None, name='pre_conv')\n",
    "    if dropout:\n",
    "        network = DropoutLayer(network, dropout)\n",
    "    # dense blocks with transitions in between\n",
    "    n = (depth - 1) // num_blocks\n",
    "    for b in range(num_blocks):\n",
    "        network = dense_block(network, n - 1, growth_rate, dropout,\n",
    "                              name_prefix='block%d' % (b + 1))\n",
    "        if b < num_blocks - 1:\n",
    "            network = transition(network, dropout,\n",
    "                                 name_prefix='block%d_trs' % (b + 1))\n",
    "    # post processing until prediction\n",
    "    network = BatchNormLayer(network, name='post_bn')\n",
    "    network = NonlinearityLayer(network, nonlinearity=rectify,\n",
    "                                name='post_relu')\n",
    "    network = GlobalPoolLayer(network, name='post_pool')\n",
    "    network = DenseLayer(network, classes, nonlinearity=softmax,\n",
    "                         W=lasagne.init.HeNormal(gain=1), name='output')\n",
    "    return network\n",
    "\n",
    "def dense_block(network, num_layers, growth_rate, dropout, name_prefix):\n",
    "    # concatenated 3x3 convolutions\n",
    "    for n in range(num_layers):\n",
    "        conv = bn_relu_conv(network, channels=growth_rate,\n",
    "                            filter_size=3, dropout=dropout,\n",
    "                            name_prefix=name_prefix + '_l%02d' % (n + 1))\n",
    "        network = ConcatLayer([network, conv], axis=1,\n",
    "                              name=name_prefix + '_l%02d_join' % (n + 1))\n",
    "    return network\n",
    "\n",
    "def transition(network, dropout, name_prefix):\n",
    "    # a transition 1x1 convolution followed by avg-pooling\n",
    "    network = bn_relu_conv(network, channels=network.output_shape[1],\n",
    "                           filter_size=1, dropout=dropout,\n",
    "                           name_prefix=name_prefix)\n",
    "    network = Pool2DLayer(network, 2, mode='average_inc_pad',\n",
    "                          name=name_prefix + '_pool')\n",
    "    return network\n",
    "\n",
    "def bn_relu_conv(network, channels, filter_size, dropout, name_prefix):\n",
    "    network = BatchNormLayer(network, name=name_prefix + '_bn')\n",
    "    network = NonlinearityLayer(network, nonlinearity=rectify,\n",
    "                                name=name_prefix + '_relu')\n",
    "    network = Conv2DLayer(network, channels, filter_size, pad='same',\n",
    "                          W=lasagne.init.HeNormal(gain='relu'),\n",
    "                          b=None, nonlinearity=None,\n",
    "                          name=name_prefix + '_conv')\n",
    "    if dropout:\n",
    "        network = DropoutLayer(network, dropout)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from viz import draw_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net= build_densenet(input_shape=(None, 1, 28, 28), input_var=None, classes=10,\n",
    "                   depth=7, first_output=16, growth_rate=3, num_blocks=2,\n",
    "                   dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_to_file(get_all_layers(net), \"jandense.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
