{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.updates import *\n",
    "from lasagne.utils import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.insert(0,'..')\n",
    "    from common import create_run_dir, plot_learn_curve\n",
    "else:\n",
    "    from ..common import create_run_dir, plot_learn_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "    sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-n', '--num_ims', default=2000, type=int,\n",
    "    help='number of total images')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "ims = mnist['data']\n",
    "\n",
    "ims.shape\n",
    "\n",
    "ims = ims.reshape(ims.shape[0],1, 28,28).astype('float64')\n",
    "\n",
    "lbls = mnist['target'].astype('int32')\n",
    "ims= ims[:args.num_ims]\n",
    "lbls = lbls[:args.num_ims]\n",
    "ims -= np.mean(ims)\n",
    "ims /= np.var(ims)\n",
    "\n",
    "num_ims = ims.shape[0]\n",
    "inds = np.arange(num_ims)\n",
    "np.random.RandomState(11).shuffle(inds)\n",
    "ims= ims[inds]\n",
    "lbls =lbls[inds]\n",
    "\n",
    "im_tr, lbl_tr, im_val, lbl_val = ims[:int(0.8*num_ims)], lbls[:int(0.8*num_ims)], \\\n",
    "                                 ims[int(0.8*num_ims):], lbls[int(0.8*num_ims):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > inputs.shape[0]:\n",
    "        batchsize=inputs.shape[0]\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net(net_cfg, args):\n",
    "    l_out = net_cfg(args)\n",
    "    X = T.tensor4('X')\n",
    "    Y= T.ivector('Y')\n",
    "    net_out = get_output(l_out, X)\n",
    "    loss = categorical_crossentropy(net_out, Y).mean()\n",
    "    net_out_det = get_output(l_out, X, deterministic=True)\n",
    "    acc = lasagne.objectives.categorical_accuracy(net_out_det,Y).mean()\n",
    "    params = get_all_params(l_out, trainable=True)\n",
    "    lr = theano.shared(floatX(args[\"learning_rate\"]))\n",
    "    if \"rmsprop\" in args:\n",
    "        updates = rmsprop(loss, params, learning_rate=lr)\n",
    "    else:\n",
    "        updates = nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "    #updates = adadelta(loss, params, learning_rate=lr)\n",
    "    #updates = rmsprop(loss, params, learning_rate=lr)\n",
    "    train_fn = theano.function([X, Y], loss, updates=updates)\n",
    "    loss_fn = theano.function([X, Y], loss)\n",
    "    val_fn = theano.function([X,Y], [loss,acc])\n",
    "    out_fn = theano.function([X], net_out_det)\n",
    "    return {\n",
    "        \"train_fn\": train_fn,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"out_fn\": out_fn,\n",
    "        \"val_fn\": val_fn,\n",
    "        \"lr\": lr,\n",
    "        \"l_out\": l_out\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dense_conv_encoder(args):\n",
    "    conv_kwargs = dict(filter_size=3, pad=1, nonlinearity=args[\"nonlinearity\"])\n",
    "    inp = InputLayer(args[\"input_shape\"])\n",
    "    conc = Conv2DLayer(inp, num_filters=args['k0'], **conv_kwargs)\n",
    "    conv_kwargs.update({'num_filters': args['k']})\n",
    "    for j in range(args['B']):\n",
    "        conc = make_dense_block(conc, args, conv_kwargs=conv_kwargs)\n",
    "        if j < args['B']:\n",
    "            conc = make_trans_layer(conc, args)\n",
    "    return conc\n",
    "\n",
    "            \n",
    "def make_dense_conv_classifier(args):  \n",
    "    conc = make_dense_conv_encoder(args)\n",
    "    conc = Pool2DLayer(conc, pool_size=2, stride=2,mode='average_exc_pad')\n",
    "    sm = DenseLayer(conc, num_units=args['num_classes'], nonlinearity=softmax)\n",
    "    for layer in get_all_layers(sm):\n",
    "        print  layer, layer.output_shape\n",
    "    print count_params(layer)\n",
    "    print sm.output_shape\n",
    "    return sm\n",
    "\n",
    "    \n",
    "\n",
    "def make_dense_block(inp, args, conv_kwargs={}):\n",
    "        conc = inp\n",
    "        block_layers = [conc]\n",
    "        for i in range(args['L']):\n",
    "            bn = BatchNormLayer(conc)\n",
    "            bn_relu = NonlinearityLayer(bn ,nonlinearity=args['nonlinearity'])\n",
    "            bn_relu_conv = Conv2DLayer(bn_relu, **conv_kwargs)\n",
    "            block_layers.append(bn_relu_conv)\n",
    "            conc = ConcatLayer(block_layers, axis=1)\n",
    "        return conc\n",
    "\n",
    "def make_trans_layer(inp,args):\n",
    "    conc = inp\n",
    "    conv = Conv2DLayer(conc, num_filters=conc.output_shape[1], filter_size=1)\n",
    "    conc = Pool2DLayer(conv, pool_size=2,stride=2, mode='average_exc_pad')\n",
    "    return conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_inverse_trans_layer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_inverse_dense_block():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_inverse(l_in, layer):\n",
    "    if isinstance(layer, Conv2DLayer):\n",
    "        return Deconv2DLayer(l_in, layer.input_shape[1], layer.filter_size, stride=layer.stride, crop=layer.pad,\n",
    "                             nonlinearity=layer.nonlinearity)\n",
    "    elif isinstance(layer, Pool2DLayer) or isinstance(layer, DenseLayer):\n",
    "        return InverseLayer(l_in, layer)\n",
    "    else:\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dense_conv_autoencoder(args):\n",
    "    conc = make_dense_conv_encoder(args)\n",
    "    conc = make_trans_layer(conc, args)\n",
    "    last_conv_shape = tuple([k if k is not None else [i] for i,k in enumerate(get_output_shape(conc,args['input_shape']))] )\n",
    "    hid_lay = DenseLayer(conc, num_units=args['num_fc_units'])\n",
    "    rec = DenseLayer(hid_lay,num_units=np.prod(last_conv_shape))\n",
    "    conc = ReshapeLayer(rec, shape=last_conv_shape)\n",
    "    \n",
    "    \n",
    "    for lay in get_all_layers(conc):\n",
    "        if not isinstance(lay)\n",
    "        bn = BatchNormLayer(conc)\n",
    "        bn_relu = NonlinearityLayer(bn ,nonlinearity=args['nonlinearity'])\n",
    "        conc = make_inverse(conc, lay)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = {\"B\":2, \"L\": 5, 'k':3, 'k0':16, \"num_classes\":10, \"input_shape\": (None,1,28,28), \"learning_rate\": 0.01, \"sigma\":0.1, \"nonlinearity\":elu, \"f\":128, \"tied\":False }\n",
    "net_cfg = get_net(make_dense_conv_classifier, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 128\n",
    "import logging\n",
    "run_dir = create_run_dir()\n",
    "try:\n",
    "    print logger\n",
    "except:\n",
    "    logger = logging.getLogger('log_train')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "logger.info(\"train size = %i, val size = %i\"%(im_tr.shape[0], im_val.shape[0])) \n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time() \n",
    "    tr_loss = 0\n",
    "    for iteration, (x, y) in enumerate(iterate_minibatches(im_tr,lbl_tr,batchsize=batch_size)):  \n",
    "        loss = net_cfg['train_fn'](x,y)\n",
    "        print loss\n",
    "        tr_loss += loss\n",
    "    \n",
    "    train_end = time.time()\n",
    "    tr_avgloss = tr_loss / (iteration + 1)\n",
    "    \n",
    "    \n",
    "    logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "    logger.info(\" epoch %i of %i train loss is %f\" % (epoch, num_epochs, tr_avgloss))\n",
    "    tr_losses.append(tr_avgloss)\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc =0 \n",
    "    for iteration, (xval, yval) in enumerate(iterate_minibatches(im_val,lbl_val,batchsize=batch_size)):\n",
    "        loss,acc = net_cfg['val_fn'](xval, yval)\n",
    "        val_acc += acc\n",
    "        val_loss += loss\n",
    "    \n",
    "    val_avgacc = val_acc / (iteration + 1) \n",
    "    val_avgloss = val_loss / (iteration + 1)   \n",
    "    logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "\n",
    "    logger.info(\" epoch %i of %i val loss is %f\" % (epoch, num_epochs, val_avgloss))\n",
    "    logger.info(\" epoch %i of %i val acc is %f percent\" % (epoch, num_epochs, 100*val_avgacc))\n",
    "    val_losses.append(val_avgloss)\n",
    "    \n",
    "    plot_learn_curve(tr_losses, val_losses, save_dir=run_dir)\n",
    "#     if epoch % 5 == 0:\n",
    "#         plot_filters(net_cfg['l_out'], save_dir=run_dir)\n",
    "#         for iteration, (x,y) in enumerate(data_iterator(batch_size=batch_size, step_size=128, days=1, month1='01', day1='01')):\n",
    "#             plot_recs(iteration,x,net_cfg=net_cfg, save_dir=run_dir)\n",
    "#             plot_clusters(iteration,x,y,net_cfg=net_cfg, save_dir=run_dir)\n",
    "#             plot_feature_maps(iteration,x,net_cfg['l_out'], save_dir=run_dir)\n",
    "#             break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
