{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.updates import *\n",
    "from lasagne.utils import *\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.insert(0,'..')\n",
    "    from common import create_run_dir, plot_learn_curve\n",
    "else:\n",
    "    from ..common import create_run_dir, plot_learn_curve\n",
    "from viz import draw_to_file\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    if batchsize > inputs.shape[0]:\n",
    "        batchsize=inputs.shape[0]\n",
    "    for start_idx in range(0,len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net_classif(net_cfg, args):\n",
    "    l_out = net_cfg(args)\n",
    "    X = T.tensor4('X')\n",
    "    Y= T.ivector('Y')\n",
    "    net_out = get_output(l_out, X)\n",
    "    loss = categorical_crossentropy(net_out, Y).mean()\n",
    "    net_out_det = get_output(l_out, X, deterministic=True)\n",
    "    acc = lasagne.objectives.categorical_accuracy(net_out_det,Y).mean()\n",
    "    params = get_all_params(l_out, trainable=True)\n",
    "    lr = theano.shared(floatX(args[\"learning_rate\"]))\n",
    "    if \"rmsprop\" in args:\n",
    "        updates = rmsprop(loss, params, learning_rate=lr)\n",
    "    else:\n",
    "        updates = nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "    #updates = adadelta(loss, params, learning_rate=lr)\n",
    "    #updates = rmsprop(loss, params, learning_rate=lr)\n",
    "    train_fn = theano.function([X, Y], loss, updates=updates)\n",
    "    loss_fn = theano.function([X, Y], loss)\n",
    "    val_fn = theano.function([X,Y], [loss,acc])\n",
    "    out_fn = theano.function([X], net_out_det)\n",
    "    return {\n",
    "        \"train_fn\": train_fn,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"out_fn\": out_fn,\n",
    "        \"val_fn\": val_fn,\n",
    "        \"lr\": lr,\n",
    "        \"l_out\": l_out\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net(net_cfg, args):\n",
    "    l_out, hid_lay = net_cfg(args)            \n",
    "    X = T.tensor4('X')\n",
    "    net_out = get_output(l_out, X)\n",
    "#     ladder_output = get_output(ladder, X)\n",
    "    # squared error loss is between the first two terms\n",
    "    loss = squared_error(net_out, X).mean()\n",
    "    sys.stderr.write(\"main loss between %s and %s\\n\" % (str(l_out.output_shape), \"X\") )\n",
    "#     if \"ladder\" in args:\n",
    "#         sys.stderr.write(\"using ladder connections for conv\\n\")        \n",
    "#         for i in range(0, len(ladder_output), 2):\n",
    "#             sys.stderr.write(\"ladder connection between %s and %s\\n\" % (str(ladder[i].output_shape), str(ladder[i+1].output_shape)) )\n",
    "#             assert ladder[i].output_shape == ladder[i+1].output_shape\n",
    "#             loss += args[\"ladder\"]*squared_error(ladder_output[i], ladder_output[i+1]).mean()\n",
    "    net_out_det = get_output(l_out, X, deterministic=True)\n",
    "    params = get_all_params(l_out, trainable=True)\n",
    "    lr = theano.shared(floatX(args[\"learning_rate\"]))\n",
    "    if \"optim\" not in args:\n",
    "        updates = nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "    else:\n",
    "        if args[\"optim\"] == \"rmsprop\":\n",
    "            updates = rmsprop(loss, params, learning_rate=lr)\n",
    "        elif args[\"optim\"] == \"adam\":\n",
    "            updates = adam(loss, params, learning_rate=lr)\n",
    "    #updates = adadelta(loss, params, learning_rate=lr)\n",
    "    #updates = rmsprop(loss, params, learning_rate=lr)\n",
    "    train_fn = theano.function([X], loss, updates=updates)\n",
    "    loss_fn = theano.function([X], loss)\n",
    "    out_fn = theano.function([X], net_out_det)\n",
    "    hid_fn = theano.function([X], get_output(hid_lay, X))\n",
    "    return {\n",
    "        \"train_fn\": train_fn,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"out_fn\": out_fn,\n",
    "        \"lr\": lr,\n",
    "        \"l_out\": l_out,\n",
    "        \"h_fn\" : hid_fn\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net_ae(net_cfg, args):\n",
    "    l_out, hid_layer = net_cfg(args)\n",
    " \n",
    "    X = T.tensor4('X')\n",
    "    Y = T.ivector('Y')\n",
    "    net_out = get_output(l_out, X)\n",
    "    hid_out = get_output(hid_layer, X)\n",
    "\n",
    "    \n",
    "    clsf_loss = get_classifier_loss(hid_layer,X,Y,args)\n",
    "    rec_loss = squared_error(net_out, X).mean()\n",
    "    if args['with_classif_loss']:\n",
    "        loss = args['lrec'] * rec_loss + clsf_loss\n",
    "        inputs = [X,Y]\n",
    "    else:\n",
    "        loss = rec_loss\n",
    "        inputs=[X]\n",
    "    params = get_all_params(l_out, trainable=True)\n",
    "    lr = theano.shared(floatX(args[\"learning_rate\"]))\n",
    "    updates = nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "    train_fn = theano.function(inputs, loss, updates=updates)\n",
    "    loss_fn = theano.function(inputs, loss)\n",
    "    out_fn = theano.function([X], net_out)\n",
    "    hid_fn = theano.function([X],hid_out)\n",
    "    return {\n",
    "        \"train_fn\": train_fn,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"out_fn\": out_fn,\n",
    "        \"lr\": lr,\n",
    "        \"l_out\": l_out,\n",
    "        \"h_fn\": hid_fn,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dense_conv_encoder(args):\n",
    "    conv_kwargs = dict(filter_size=3, pad=1, nonlinearity=args['nonlinearity'], W=lasagne.init.HeNormal())\n",
    "    inp = InputLayer(args[\"input_shape\"])\n",
    "    if \"sigma\" in args:\n",
    "        inp = GaussianNoiseLayer(inp, sigma=args['sigma'])\n",
    "    conc = Conv2DLayer(inp, num_filters=args['k0'], **conv_kwargs)\n",
    "    conv_kwargs.update({'num_filters': args['k'], 'nonlinearity': None})\n",
    "    for j in range(args['B']):\n",
    "        conc = make_dense_block(conc, args, conv_kwargs=conv_kwargs)\n",
    "        if j < args['B'] - 1:\n",
    "            conc = make_trans_layer(conc, args)\n",
    "    bn = BatchNormLayer(conc)\n",
    "    bn_relu = NonlinearityLayer(bn ,nonlinearity=args['nonlinearity'])\n",
    "    conc = GlobalPoolLayer(bn_relu)\n",
    "    return conc\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def make_dense_conv_classifier(args):  \n",
    "    conc = make_dense_conv_encoder(args)\n",
    "    sm = DenseLayer(conc, num_units=args['num_classes'], nonlinearity=softmax)\n",
    "    for layer in get_all_layers(sm):\n",
    "        print  layer, layer.output_shape\n",
    "    print count_params(layer)\n",
    "    print sm.output_shape\n",
    "    return sm\n",
    "\n",
    "    \n",
    "\n",
    "def make_dense_block(inp, args, conv_kwargs={}):\n",
    "        conc = inp\n",
    "        block_layers = [conc]\n",
    "        for i in range(args['L']):\n",
    "            bn = BatchNormLayer(conc)\n",
    "            bn_relu = NonlinearityLayer(bn ,nonlinearity=args['nonlinearity'])\n",
    "            bn_relu_conv = Conv2DLayer(bn_relu, **conv_kwargs)\n",
    "            block_layers.append(bn_relu_conv)\n",
    "            conc = ConcatLayer([conc, bn_relu_conv], axis=1)\n",
    "        return conc\n",
    "\n",
    "def make_trans_layer(inp,args):\n",
    "    conc = inp\n",
    "    bn = BatchNormLayer(conc)\n",
    "    bn_relu = NonlinearityLayer(bn ,nonlinearity=args['nonlinearity'])\n",
    "    conv = Conv2DLayer(bn_relu, num_filters=conc.output_shape[1], filter_size=1)\n",
    "    conc = Pool2DLayer(conv, pool_size=2,stride=2, mode='average_exc_pad')\n",
    "    return conc\n",
    "\n",
    "def make_inverse_trans_layer(inp, layer,args):\n",
    "    conc = inp\n",
    "    #because trans layers are 2 layerrs log\n",
    "    for lay in get_all_layers(layer)[::-1][:2]:\n",
    "        conc = make_inverse(conc, lay,args)\n",
    "    \n",
    "    #return whole network and next layer we are going to invert\n",
    "    return conc, lay.input_layer\n",
    "\n",
    "def make_inverse_dense_block(inp, layer, args):\n",
    "    conc = inp\n",
    "\n",
    "    #3 layers per comp unit and args['L'] units per block\n",
    "    for lay in get_all_layers(layer)[::-1][:4*args['L']]:\n",
    "        if isinstance(lay, ConcatLayer):\n",
    "            conc = make_inverse(conc,lay,args)\n",
    "            \n",
    "#             conc = BatchNormLayer(conc)\n",
    "#             conc = make_inverse(conc, lay, filters=args['k'])\n",
    "#             conc = NonlinearityLayer(conc, nonlinearity=args['nonlinearity'])\n",
    "#             block_layers.append(conc)\n",
    "#             if args['concat_inver']:\n",
    "#                 conc = ConcatLayer(block_layers,axis=1)\n",
    "            \n",
    "    return conc, lay.input_layer\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "def make_inverse(l_in, layer,args):\n",
    "    \n",
    "    if isinstance(layer, Conv2DLayer):\n",
    "        if 'dec_batch_norm' in args and args['dec_batch_norm']:\n",
    "            l_in = batch_norm(l_in)\n",
    "        if 'tied_weights' in args and args['tied_weights']:\n",
    "            l_in = InverseLayer(l_in,layer)\n",
    "        else:\n",
    "            l_in = Deconv2DLayer(l_in, layer.input_shape[1], layer.filter_size, stride=layer.stride, crop=layer.pad, nonlinearity=layer.nonlinearity)\n",
    "        return NonlinearityLayer(l_in, nonlinearity=args['nonlinearity'])\n",
    "        \n",
    "#         if filters is None:\n",
    "#             filters = layer.input_shape[1]\n",
    "#         return \n",
    "    elif isinstance(layer, Pool2DLayer) or isinstance(layer, GlobalPoolLayer) or isinstance(layer, DenseLayer):\n",
    "        return InverseLayer(l_in, layer)\n",
    "    elif isinstance(layer, ConcatLayer):\n",
    "        first_input_shape = layer.input_shapes[0][layer.axis]\n",
    "        return SliceLayer(l_in,indices=slice(0,first_input_shape), axis=layer.axis)\n",
    "               #SliceLayer(l_in,indices=slice(first_input_shape, -1), axis=layer.axis )\n",
    "    else:\n",
    "        return l_in\n",
    "\n",
    "def make_dense_conv_autoencoder(args):\n",
    "    conc = make_dense_conv_encoder(args)\n",
    "    hid_lay = DenseLayer(conc, num_units=args['num_fc_units'])\n",
    "    conc = hid_lay\n",
    "    for layer in get_all_layers(conc)[::-1]:\n",
    "        if isinstance(layer, ConcatLayer):\n",
    "            break\n",
    "        else:\n",
    "            conc = make_inverse(conc,layer,args)\n",
    "        \n",
    "    for i in range(args['B']):\n",
    "        conc, layer = make_inverse_dense_block(conc, layer, args)\n",
    "        if i < args['B'] - 1:\n",
    "                conc, layer = make_inverse_trans_layer(conc, layer, args)\n",
    "    \n",
    "        \n",
    "\n",
    "    for lay in get_all_layers(layer)[::-1]:\n",
    "        if isinstance(lay, InputLayer):\n",
    "            break\n",
    "        args['nonlinearity'] = tanh\n",
    "        conc = make_inverse(conc, lay,args)\n",
    "        \n",
    "    for layer_ in get_all_layers(conc):\n",
    "            print  layer_, layer_.output_shape\n",
    "    print count_params(layer_)\n",
    "    \n",
    "    \n",
    "    return conc, hid_lay\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "    sys.argv=sys.argv[:1]\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-n', '--num_ims', default=5000, type=int,help='number of total images')\n",
    "parser.add_argument('-L', '--num_layers_per_block', default=2, type=int)\n",
    "parser.add_argument('-B', '--num_blocks', default=2, type=int)\n",
    "parser.add_argument('-k', '--num_filters_per_conv', default=3, type=int)\n",
    "parser.add_argument('-K', '--num_filters_in_first_conv', default=16, type=int)\n",
    "parser.add_argument('-t', '--tied_weights', dest='tied_weights', action='store_true')\n",
    "parser.set_defaults(tied_weights=False)\n",
    "parser.add_argument('-b', '--batch_norm_dec', dest='dec_batch_norm', action='store_true')\n",
    "parser.set_defaults(dec_batch_norm=False)\n",
    "parser.add_argument('--fc', default=100, type=int)\n",
    "parser.add_argument('-s', '--sigma', default=0.5, type=float)\n",
    "parser.add_argument( '--batchsize', default=128, type=int)\n",
    "parser.add_argument( '--num_epochs', default=1000, type=int)\n",
    "parser.add_argument( '--learn_rate', default=0.1, type=float)\n",
    "pargs = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "ims = mnist['data']\n",
    "\n",
    "ims.shape\n",
    "\n",
    "ims = ims.reshape(ims.shape[0],1, 28,28).astype('float64')\n",
    "\n",
    "lbls = mnist['target'].astype('int32')\n",
    "ims= ims[:pargs.num_ims]\n",
    "lbls = lbls[:pargs.num_ims]\n",
    "ims -= np.mean(ims)\n",
    "ims /= np.var(ims)\n",
    "\n",
    "num_ims = ims.shape[0]\n",
    "inds = np.arange(num_ims)\n",
    "np.random.RandomState(11).shuffle(inds)\n",
    "ims= ims[inds]\n",
    "lbls =lbls[inds]\n",
    "\n",
    "im_tr, lbl_tr, im_val, lbl_val = ims[:int(0.8*num_ims)], lbls[:int(0.8*num_ims)], \\\n",
    "                                 ims[int(0.8*num_ims):], lbls[int(0.8*num_ims):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = {\"B\":pargs.num_blocks, \"L\": pargs.num_layers_per_block, 'k':pargs.num_filters_per_conv,\n",
    "        'k0':pargs.num_filters_in_first_conv, \"num_classes\":10,'dec_batch_norm':pargs.dec_batch_norm,\n",
    "        'tied_weights':pargs.tied_weights, \"num_fc_units\":pargs.fc,\n",
    "        \"input_shape\": (None,1,28,28), \"learning_rate\": pargs.learn_rate,\n",
    "        \"sigma\":pargs.sigma, \"nonlinearity\":elu, \"f\":128, 'schedule': {100:0.01, 1000: 0.001}}\n",
    "#net_cfg = get_net(make_dense_conv_classifier, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_clusters(i,x,y,net_cfg, save_dir='.'):\n",
    "    hid_L = net_cfg['h_fn'](x)\n",
    "    ts = TSNE().fit_transform(hid_L)\n",
    "    plt.clf()\n",
    "    plt.scatter(ts[:,0], ts[:,1], c=y)\n",
    "    plt.savefig(save_dir + '/cluster_%i.png'%(i))\n",
    "    plt.clf()\n",
    "\n",
    "def plot_recs(i,x,net_cfg, save_dir='.'):\n",
    "    ind = np.random.randint(0,x.shape[0], size=(1,))\n",
    "    #print x.shape\n",
    "    im = x[ind]\n",
    "    #print im.shape\n",
    "    rec = net_cfg['out_fn'](im)\n",
    "    ch=1\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.clf()\n",
    "    for (p_im, p_rec) in zip(im[0],rec[0]):\n",
    "        p1 = plt.subplot(im.shape[1],2, ch )\n",
    "        p2 = plt.subplot(im.shape[1],2, ch + 1)\n",
    "        p1.imshow(p_im)\n",
    "        p2.imshow(p_rec)\n",
    "        ch = ch+2\n",
    "    #pass\n",
    "    plt.savefig(save_dir +'/recs_%i' %(i))\n",
    "\n",
    "def plot_filters(network,num_filter_channels_to_plot=1, save_dir='.'):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.clf()\n",
    "    lay_ind = 0\n",
    "    num_channels_to_plot = num_filter_channels_to_plot\n",
    "    convlayers = [layer for layer in get_all_layers(network) if isinstance(layer, Conv2DLayer)]\n",
    "    num_layers = len(convlayers)\n",
    "    spind = 1 \n",
    "    for layer in convlayers:\n",
    "        filters = layer.get_params()[0].eval()\n",
    "        #pick a random filter\n",
    "        filt = filters[np.random.randint(0,filters.shape[0])]\n",
    "        for ch_ind in range(num_channels_to_plot):\n",
    "            p1 = plt.subplot(num_layers,num_channels_to_plot, spind )\n",
    "            p1.imshow(filt[ch_ind], cmap=\"gray\")\n",
    "            spind = spind + 1\n",
    "    \n",
    "    #pass\n",
    "    plt.savefig(save_dir +'/filters.png')\n",
    "            \n",
    "        \n",
    "def plot_feature_maps(i, x, network, save_dir='.'):\n",
    "    plt.figure(figsize=(30,30))\n",
    "    plt.clf()\n",
    "    ind = np.random.randint(0,x.shape[0])\n",
    "\n",
    "    im = x[ind]\n",
    "    convlayers = [layer for layer in get_all_layers(network) if isinstance(layer,Conv2DLayer) or \\\n",
    "                  isinstance(layer, TransposedConv2DLayer) or isinstance(layer,Pool2DLayer)]\n",
    "    num_layers = len(convlayers)\n",
    "    spind = 1 \n",
    "    num_fmaps_to_plot = 1\n",
    "    for ch in range(im.shape[0]):\n",
    "        p1 = plt.subplot(num_layers + 2,num_fmaps_to_plot, spind )\n",
    "        p1.imshow(im[ch], cmap=\"gray\")\n",
    "        spind = spind + 1\n",
    "    spind = num_fmaps_to_plot +1\n",
    "    for layer in convlayers:\n",
    "        # shape is batch_size, num_filters, x,y \n",
    "        fmaps = get_output(layer,x ).eval()\n",
    "        for fmap_ind in range(num_fmaps_to_plot):\n",
    "            p1 = plt.subplot(num_layers + 2,num_fmaps_to_plot, spind )\n",
    "            p1.imshow(fmaps[ind][fmap_ind], cmap=\"gray\")\n",
    "            spind = spind + 1\n",
    "    \n",
    "    out = get_output(network,x).eval()\n",
    "    for ch in range(out.shape[1]):\n",
    "        p1 = plt.subplot(num_layers + 2,num_fmaps_to_plot, spind )\n",
    "        p1.imshow(out[ind][ch], cmap=\"gray\")\n",
    "        spind = spind + 1\n",
    "    \n",
    "    #pass\n",
    "    plt.savefig(save_dir +'/fmaps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train size = 4000, val size = 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lasagne.layers.input.InputLayer object at 0x2af0cc30a9d0> (None, 1, 28, 28)\n",
      "<lasagne.layers.noise.GaussianNoiseLayer object at 0x2af0d0720750> (None, 1, 28, 28)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d07207d0> (None, 16, 28, 28)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d0746d10> (None, 16, 28, 28)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d075a910> (None, 16, 28, 28)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d075aa50> (None, 3, 28, 28)\n",
      "<lasagne.layers.merge.ConcatLayer object at 0x2af0d075ab90> (None, 19, 28, 28)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d075ad10> (None, 19, 28, 28)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d076b190> (None, 19, 28, 28)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d076b2d0> (None, 3, 28, 28)\n",
      "<lasagne.layers.merge.ConcatLayer object at 0x2af0d076b410> (None, 22, 28, 28)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d0746cd0> (None, 22, 28, 28)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d076b990> (None, 22, 28, 28)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d076bad0> (None, 22, 28, 28)\n",
      "<lasagne.layers.pool.Pool2DLayer object at 0x2af0d076bc10> (None, 22, 14, 14)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d076bdd0> (None, 22, 14, 14)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d077a250> (None, 22, 14, 14)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d077a390> (None, 3, 14, 14)\n",
      "<lasagne.layers.merge.ConcatLayer object at 0x2af0d077a4d0> (None, 25, 14, 14)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d077a650> (None, 25, 14, 14)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d077aa90> (None, 25, 14, 14)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2af0d077abd0> (None, 3, 14, 14)\n",
      "<lasagne.layers.merge.ConcatLayer object at 0x2af0d077ad10> (None, 28, 14, 14)\n",
      "<lasagne.layers.normalization.BatchNormLayer object at 0x2af0d0746b50> (None, 28, 14, 14)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d078a290> (None, 28, 14, 14)\n",
      "<lasagne.layers.pool.GlobalPoolLayer object at 0x2af0d078a3d0> (None, 28)\n",
      "<lasagne.layers.dense.DenseLayer object at 0x2af09a492d90> (None, 100)\n",
      "<lasagne.layers.special.InverseLayer object at 0x2af0d078aa10> (None, 28)\n",
      "<lasagne.layers.special.InverseLayer object at 0x2af0d078aa50> (None, 28, 14, 14)\n",
      "<lasagne.layers.shape.SliceLayer object at 0x2af0d078aad0> (None, 25, 14, 14)\n",
      "<lasagne.layers.shape.SliceLayer object at 0x2af0d078ab10> (None, 22, 14, 14)\n",
      "<lasagne.layers.special.InverseLayer object at 0x2af0d078ab50> (None, 22, 28, 28)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2af0d078ab90> (None, 22, 28, 28)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d078acd0> (None, 22, 28, 28)\n",
      "<lasagne.layers.shape.SliceLayer object at 0x2af0d078ae50> (None, 19, 28, 28)\n",
      "<lasagne.layers.shape.SliceLayer object at 0x2af0d078ae90> (None, 16, 28, 28)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2af0d078aa90> (None, 1, 28, 28)\n",
      "<lasagne.layers.special.NonlinearityLayer object at 0x2af0d078afd0> (None, 1, 28, 28)\n",
      "6971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main loss between (None, 1, 28, 28) and X\n"
     ]
    }
   ],
   "source": [
    "run_dir = create_run_dir()\n",
    "import logging\n",
    "try:\n",
    "    print logger\n",
    "except:\n",
    "    logger = logging.getLogger('log_train')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler('%s/results.txt'%(run_dir))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "logger.info(\"train size = %i, val size = %i\"%(im_tr.shape[0], im_val.shape[0])) \n",
    "\n",
    "net_cfg = get_net(make_dense_conv_autoencoder,args)\n",
    "\n",
    "draw_to_file(get_all_layers(net_cfg['l_out']), run_dir + \"/network_topo.eps\")\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(pargs.num_epochs):\n",
    "    start = time.time() \n",
    "    tr_loss = 0\n",
    "    for iteration, (x, y) in enumerate(iterate_minibatches(im_tr,lbl_tr,batchsize=pargs.batchsize)):  \n",
    "        loss = net_cfg['train_fn'](x)\n",
    "        print loss\n",
    "        tr_loss += loss\n",
    "    \n",
    "    train_end = time.time()\n",
    "    tr_avgloss = tr_loss / (iteration + 1)\n",
    "    \n",
    "    \n",
    "    logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "    logger.info(\" epoch %i of %i train loss is %f\" % (epoch, pargs.num_epochs, tr_avgloss))\n",
    "    tr_losses.append(tr_avgloss)\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc =0 \n",
    "    for iteration, (xval, yval) in enumerate(iterate_minibatches(im_val,lbl_val,batchsize=pargs.batchsize)):\n",
    "        loss = net_cfg['loss_fn'](xval)\n",
    "        #val_acc += acc\n",
    "        val_loss += loss\n",
    "    \n",
    "    #val_avgacc = val_acc / (iteration + 1) \n",
    "    val_avgloss = val_loss / (iteration + 1)   \n",
    "    logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "\n",
    "    logger.info(\" epoch %i of %i val loss is %f\" % (epoch, pargs.num_epochs, val_avgloss))\n",
    "    #logger.info(\" epoch %i of %i val acc is %f percent\" % (epoch, num_epochs, 100*val_avgacc))\n",
    "    val_losses.append(val_avgloss)\n",
    "    \n",
    "    if epoch in args['schedule']:\n",
    "        net_cfg['lr'] = args['schedule'][epoch]\n",
    "    \n",
    "    plot_learn_curve(tr_losses, val_losses, save_dir=run_dir)\n",
    "    if epoch % 1 == 0:\n",
    "        plot_filters(net_cfg['l_out'], save_dir=run_dir)\n",
    "        for iteration, (x,y) in enumerate(iterate_minibatches(im_tr,lbl_tr,batchsize=pargs.batchsize)):\n",
    "            plot_recs(iteration, x, net_cfg=net_cfg, save_dir=run_dir)\n",
    "            plot_clusters(iteration, x, y, net_cfg=net_cfg, save_dir=run_dir)\n",
    "            plot_feature_maps(iteration,x,net_cfg['l_out'], save_dir=run_dir)\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearning (fast)",
   "language": "python",
   "name": "deeplearning_fast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
