{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.updates import *\n",
    "from lasagne.utils import *\n",
    "from lasagne.init import *\n",
    "import numpy as np\n",
    "#import cPickle as pickle\n",
    "#import gzip\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.insert(0,'..')\n",
    "    from common import *\n",
    "else:\n",
    "    from ..common import *\n",
    "import time\n",
    "import logging\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_net(net_cfg, args):\n",
    "    l_out, hid_layer = net_cfg(args)\n",
    " \n",
    "    X = T.tensor4('X')\n",
    "    Y = T.ivector('Y')\n",
    "    net_out = get_output(l_out, X)\n",
    "    hid_out = get_output(hid_layer, X)\n",
    "\n",
    "    \n",
    "    clsf_loss = get_classifier_loss(hid_layer,X,Y,args)\n",
    "    rec_loss = squared_error(net_out, X).mean()\n",
    "    if args['with_classif_loss']:\n",
    "        loss = args['lrec'] * rec_loss + clsf_loss\n",
    "        inputs = [X,Y]\n",
    "    else:\n",
    "        loss = rec_loss\n",
    "        inputs=[X]\n",
    "    params = get_all_params(l_out, trainable=True)\n",
    "    lr = theano.shared(floatX(args[\"learning_rate\"]))\n",
    "    updates = nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "    train_fn = theano.function(inputs, loss, updates=updates)\n",
    "    loss_fn = theano.function(inputs, loss)\n",
    "    out_fn = theano.function([X], net_out)\n",
    "    hid_fn = theano.function([X],hid_out)\n",
    "    return {\n",
    "        \"train_fn\": train_fn,\n",
    "        \"loss_fn\": loss_fn,\n",
    "        \"out_fn\": out_fn,\n",
    "        \"lr\": lr,\n",
    "        \"l_out\": l_out,\n",
    "        \"h_fn\": hid_fn,\n",
    "\n",
    "    }\n",
    "\n",
    "def get_classifier_loss(hid_layer,x,y, args):\n",
    "    \n",
    "    clsf = DenseLayer(hid_layer, num_units=args['num_classes'], nonlinearity=softmax)\n",
    "    label_inds = y > -3\n",
    "    \n",
    "    #get x's with labels\n",
    "    x_lbl = x[label_inds.nonzero()]\n",
    "    y_lbl = y[label_inds.nonzero()]\n",
    "    y_lbl = y_lbl + 2\n",
    "    \n",
    "    clsf_out = get_output(clsf, x_lbl)\n",
    "    clsf_loss = categorical_crossentropy(clsf_out, y_lbl).mean()\n",
    "    return clsf_loss\n",
    "\n",
    "def autoencoder_basic_32(args):\n",
    "    conv_kwargs = {'nonlinearity': rectify, 'W': HeNormal()}\n",
    "    net = InputLayer(args['shape'])\n",
    "    net = GaussianNoiseLayer(net, args[\"sigma\"])\n",
    "    for i in range(5):\n",
    "        net = Conv2DLayer(net, num_filters=args[\"nfilters\"], filter_size=2,stride=2, **conv_kwargs)\n",
    "        #net = MaxPool2DLayer(net, pool_size=2)\n",
    "    last_conv_shape = tuple([k if k is not None else [i] for i,k in enumerate(get_output_shape(net,args['shape']))] )\n",
    "    \n",
    "    hid_layer = DenseLayer(net, num_units=args['code_size'], **conv_kwargs)\n",
    "    net = DenseLayer(hid_layer, num_units=np.prod(last_conv_shape[1:]))\n",
    "    net = ReshapeLayer(net, shape=last_conv_shape)\n",
    "    \n",
    "    for layer in get_all_layers(net)[::-1]:\n",
    "        if isinstance(layer, MaxPool2DLayer):\n",
    "            net = InverseLayer(net, layer)\n",
    "            \n",
    "        if isinstance(layer, Conv2DLayer):\n",
    "            conv_dict = {key:getattr(layer, key) for key in [\"stride\", \"pad\", \"num_filters\", \"filter_size\"]}\n",
    "            conv_dict['crop'] = conv_dict['pad']\n",
    "            del conv_dict['pad']\n",
    "            \n",
    "            if not isinstance(layer.input_layer,Conv2DLayer):\n",
    "                conv_dict['num_filters'] = args[\"shape\"][1]\n",
    "                conv_dict['nonlinearity'] = linear\n",
    "            net = Deconv2DLayer(net, **conv_dict)\n",
    "\n",
    "    for layer in get_all_layers(net):\n",
    "        logger.info(str(layer) + str(layer.output_shape))\n",
    "    print count_params(layer)\n",
    "    return net, hid_layer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def plot_learn_curve(tr_losses, val_losses, save_dir='.'):\n",
    "#     plt.clf()\n",
    "#     plt.plot(tr_losses)\n",
    "#     plt.plot(val_losses)\n",
    "#     plt.savefig(save_dir + '/learn_curve.png')\n",
    "#     plt.clf()\n",
    "    \n",
    "# def plot_clusters(i,x,y, save_dir='.'):\n",
    "#     x = np.squeeze(x)\n",
    "#     hid_L = net_cfg['h_fn'](x)\n",
    "#     ts = TSNE().fit_transform(hid_L)\n",
    "#     plt.clf()\n",
    "#     plt.scatter(ts[:,0], ts[:,1], c=y)\n",
    "#     plt.savefig(save_dir + '/cluster_%i.png'%(i))\n",
    "#     plt.clf()\n",
    "\n",
    "# def plot_recs(i,x,net_cfg, save_dir='.'):\n",
    "#     ind = np.random.randint(0,x.shape[0], size=(1,))\n",
    "#     x=np.squeeze(x)\n",
    "#     #print x.shape\n",
    "#     im = x[ind]\n",
    "#     #print im.shape\n",
    "#     rec = net_cfg['out_fn'](im)\n",
    "#     ch=1\n",
    "#     plt.figure(figsize=(30,30))\n",
    "#     plt.clf()\n",
    "#     for (p_im, p_rec) in zip(im[0],rec[0]):\n",
    "#         p1 = plt.subplot(im.shape[1],2, ch )\n",
    "#         p2 = plt.subplot(im.shape[1],2, ch + 1)\n",
    "#         p1.imshow(p_im)\n",
    "#         p2.imshow(p_rec)\n",
    "#         ch = ch+2\n",
    "#     #plt.show()\n",
    "#     plt.savefig(save_dir +'/recs_%i' %(i))\n",
    "\n",
    "# def plot_filters(network, save_dir='.'):\n",
    "#     plt.figure(figsize=(30,30))\n",
    "#     plt.clf()\n",
    "#     lay_ind = 0\n",
    "#     num_channels_to_plot = 16\n",
    "#     convlayers = [layer for layer in get_all_layers(network) if isinstance(layer, Conv2DLayer)]\n",
    "#     num_layers = len(convlayers)\n",
    "#     spind = 1 \n",
    "#     for layer in convlayers:\n",
    "#         filters = layer.get_params()[0].eval()\n",
    "#         #pick a random filter\n",
    "#         filt = filters[np.random.randint(0,filters.shape[0])]\n",
    "#         for ch_ind in range(num_channels_to_plot):\n",
    "#             p1 = plt.subplot(num_layers,num_channels_to_plot, spind )\n",
    "#             p1.imshow(filt[ch_ind], cmap=\"gray\")\n",
    "#             spind = spind + 1\n",
    "    \n",
    "#     #plt.show()\n",
    "#     plt.savefig(save_dir +'/filters.png')\n",
    "            \n",
    "        \n",
    "# def plot_feature_maps(i, x, network, save_dir='.'):\n",
    "#     plt.figure(figsize=(30,30))\n",
    "#     plt.clf()\n",
    "#     ind = np.random.randint(0,x.shape[0])\n",
    "#     x=np.squeeze(x)\n",
    "\n",
    "#     im = x[ind]\n",
    "#     convlayers = [layer for layer in get_all_layers(network) if not isinstance(layer,DenseLayer)]\n",
    "#     num_layers = len(convlayers)\n",
    "#     spind = 1 \n",
    "#     num_fmaps_to_plot = 16\n",
    "#     for ch in range(num_fmaps_to_plot):\n",
    "#         p1 = plt.subplot(num_layers + 1,num_fmaps_to_plot, spind )\n",
    "#         p1.imshow(im[ch])\n",
    "#         spind = spind + 1\n",
    "      \n",
    "#     for layer in convlayers:\n",
    "#         # shape is batch_size, num_filters, x,y \n",
    "#         fmaps = get_output(layer,x ).eval()\n",
    "#         for fmap_ind in range(num_fmaps_to_plot):\n",
    "#             p1 = plt.subplot(num_layers + 1,num_fmaps_to_plot, spind )\n",
    "#             p1.imshow(fmaps[ind][fmap_ind])\n",
    "#             spind = spind + 1\n",
    "    \n",
    "#     #plt.show()\n",
    "#     plt.savefig(save_dir +'/fmaps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<lasagne.layers.input.InputLayer object at 0x2b5bfe0e4f90>(None, 16, 128, 128)\n",
      "<lasagne.layers.noise.GaussianNoiseLayer object at 0x2b5c279a1650>(None, 16, 128, 128)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b5c279a1710>(None, 64, 64, 64)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b5c27a06a90>(None, 64, 32, 32)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b5c27a06d10>(None, 64, 16, 16)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b5c27a06f90>(None, 64, 8, 8)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x2b5c27a1a250>(None, 64, 4, 4)\n",
      "<lasagne.layers.dense.DenseLayer object at 0x2b5c279a16d0>(None, 30)\n",
      "<lasagne.layers.dense.DenseLayer object at 0x2b5c27a1aa90>(None, 1024)\n",
      "<lasagne.layers.shape.ReshapeLayer object at 0x2b5c27a1ad10>(None, 64, 4, 4)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2b5c27a1aed0>(None, 64, 8, 8)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2b5c27a2b050>(None, 64, 16, 16)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2b5c27a2b2d0>(None, 64, 32, 32)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2b5c27a2b550>(None, 64, 64, 64)\n",
      "<lasagne.layers.conv.TransposedConv2DLayer object at 0x2b5c27a2b7d0>(None, 16, 128, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202350\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "batch_size = 128\n",
    "\n",
    "run_dir = create_run_dir()\n",
    "try:\n",
    "    print logger\n",
    "except:\n",
    "    logger = logging.getLogger('log_train')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    fh = logging.FileHandler('%s/training.log'%(run_dir))\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    \n",
    "args = { \"learning_rate\": 0.01, \"sigma\":0.1, \"shape\": (None,16,128,128),\n",
    "        'code_size': 16384 , 'nfilters': 128, 'lrec': 1, 'num_classes': 3, \"with_classif_loss\": False }\n",
    "net_cfg = get_net(autoencoder_basic_32, args)\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_iterator = data_iterator(batch_size=batch_size, step_size=128, days=1, month1='01', day1='01')\n",
    "    val_iterator = data_iterator(batch_size=batch_size, step_size=128, days=1, month1='10', day1='28')\n",
    "    start = time.time() \n",
    "    tr_loss = 0\n",
    "    for iteration, (x, y) in enumerate(tr_iterator):\n",
    "        #print iteration\n",
    "        x = np.squeeze(x)\n",
    "\n",
    "        \n",
    "        loss = net_cfg['train_fn'](x)\n",
    "        tr_loss += loss\n",
    "    \n",
    "    train_end = time.time()\n",
    "    tr_avgloss = tr_loss / (iteration + 1)\n",
    "    \n",
    "    \n",
    "    logger.info(\"train time : %5.2f seconds\" % (train_end - start))\n",
    "    logger.info(\" epoch %i of %i train loss is %f\" % (epoch, num_epochs, tr_avgloss))\n",
    "    tr_losses.append(tr_avgloss)\n",
    "    \n",
    "    val_loss = 0\n",
    "\n",
    "    for iteration, (xval, yval) in enumerate(val_iterator):\n",
    "        xval = np.squeeze(xval)\n",
    "        loss = net_cfg['loss_fn'](xval)\n",
    "        val_loss += loss\n",
    "    \n",
    "    val_avgloss = val_loss / (iteration + 1)   \n",
    "    logger.info(\"val time : %5.2f seconds\" % (time.time() - train_end))\n",
    "\n",
    "    logger.info(\" epoch %i of %i val loss is %f\" % (epoch, num_epochs, val_avgloss))\n",
    "    val_losses.append(val_avgloss)\n",
    "    \n",
    "    plot_learn_curve(tr_losses, val_losses, save_dir=run_dir)\n",
    "    if epoch % 5 == 0:\n",
    "        plot_filters(net_cfg['l_out'], save_dir=run_dir)\n",
    "        for iteration, (x,y) in enumerate(data_iterator(batch_size=batch_size, step_size=128, days=1, month1='01', day1='01')):\n",
    "            plot_recs(iteration,x,net_cfg=net_cfg, save_dir=run_dir)\n",
    "            plot_clusters(iteration,x,y,net_cfg=net_cfg, save_dir=run_dir)\n",
    "            plot_feature_maps(iteration,x,net_cfg['l_out'], save_dir=run_dir)\n",
    "            break;\n",
    "        \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning (edison)",
   "language": "python",
   "name": "deeplearning_edison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
